{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import librosa\n",
    "#import librosa.display\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#from matplotlib.pyplot import specgram\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import librosa\n",
    "#import librosa.display\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#from matplotlib.pyplot import specgram\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (6078, 272)\n",
      "6078 train samples\n",
      "642 test samples\n"
     ]
    }
   ],
   "source": [
    "emotionsDf = pd.read_csv('emotionsDfV1.csv')\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "shuffledDf = shuffle(emotionsDf)\n",
    "\n",
    "divider = np.random.rand(len(shuffledDf)) < 0.9\n",
    "train = shuffledDf[divider]\n",
    "test = shuffledDf[~divider]\n",
    "\n",
    "trainfeatures = train.iloc[:, :-1]\n",
    "trainlabels = train.iloc[:, -1:]\n",
    "\n",
    "testfeatures = test.iloc[:, :-1]\n",
    "testlabels = test.iloc[:, -1:]\n",
    "\n",
    "x_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabels)\n",
    "x_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabels)\n",
    "\n",
    "#flattening the arrays\n",
    "y_train = np.hstack(y_train)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "#convert class vectors to binary class matrices.\n",
    "num_classes = 7\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train = np.expand_dims(x_train, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6078 samples, validate on 642 samples\n",
      "Epoch 1/100\n",
      "6078/6078 [==============================] - 4s 637us/step - loss: 17228.8408 - acc: 0.1570 - val_loss: 17211.8122 - val_acc: 0.1636\n",
      "Epoch 2/100\n",
      "6078/6078 [==============================] - 3s 497us/step - loss: 17197.2056 - acc: 0.1672 - val_loss: 17180.2338 - val_acc: 0.2087\n",
      "Epoch 3/100\n",
      "6078/6078 [==============================] - 3s 553us/step - loss: 17165.7267 - acc: 0.1700 - val_loss: 17148.6083 - val_acc: 0.2212\n",
      "Epoch 4/100\n",
      "6078/6078 [==============================] - 4s 585us/step - loss: 17134.2768 - acc: 0.1698 - val_loss: 17116.9304 - val_acc: 0.2259\n",
      "Epoch 5/100\n",
      "6078/6078 [==============================] - 3s 488us/step - loss: 17102.9593 - acc: 0.1782 - val_loss: 17085.1157 - val_acc: 0.2212\n",
      "Epoch 6/100\n",
      "6078/6078 [==============================] - 4s 594us/step - loss: 17071.2784 - acc: 0.1918 - val_loss: 17053.4126 - val_acc: 0.2196\n",
      "Epoch 7/100\n",
      "6078/6078 [==============================] - 3s 510us/step - loss: 17040.1592 - acc: 0.1899 - val_loss: 17022.0413 - val_acc: 0.2025\n",
      "Epoch 8/100\n",
      "6078/6078 [==============================] - 3s 573us/step - loss: 17008.9989 - acc: 0.2053 - val_loss: 16991.1100 - val_acc: 0.2212\n",
      "Epoch 9/100\n",
      "6078/6078 [==============================] - 3s 575us/step - loss: 16978.2455 - acc: 0.2022 - val_loss: 16960.3853 - val_acc: 0.2368\n",
      "Epoch 10/100\n",
      "6078/6078 [==============================] - 3s 513us/step - loss: 16947.1268 - acc: 0.2131 - val_loss: 16929.6975 - val_acc: 0.2383\n",
      "Epoch 11/100\n",
      "6078/6078 [==============================] - 3s 519us/step - loss: 16916.4304 - acc: 0.2155 - val_loss: 16899.0057 - val_acc: 0.2445\n",
      "Epoch 12/100\n",
      "6078/6078 [==============================] - 3s 523us/step - loss: 16885.6933 - acc: 0.2083 - val_loss: 16868.3052 - val_acc: 0.2430\n",
      "Epoch 13/100\n",
      "6078/6078 [==============================] - 3s 499us/step - loss: 16854.6550 - acc: 0.2239 - val_loss: 16837.6596 - val_acc: 0.2414\n",
      "Epoch 14/100\n",
      "6078/6078 [==============================] - 3s 575us/step - loss: 16824.1442 - acc: 0.2052 - val_loss: 16807.0578 - val_acc: 0.2461\n",
      "Epoch 15/100\n",
      "6078/6078 [==============================] - 3s 493us/step - loss: 16793.5551 - acc: 0.2098 - val_loss: 16776.4319 - val_acc: 0.2383\n",
      "Epoch 16/100\n",
      "6078/6078 [==============================] - 3s 511us/step - loss: 16762.7856 - acc: 0.2076 - val_loss: 16745.8903 - val_acc: 0.2414\n",
      "Epoch 17/100\n",
      "6078/6078 [==============================] - 3s 526us/step - loss: 16732.2807 - acc: 0.2154 - val_loss: 16715.3921 - val_acc: 0.2399\n",
      "Epoch 18/100\n",
      "6078/6078 [==============================] - 4s 595us/step - loss: 16701.7965 - acc: 0.2017 - val_loss: 16684.9206 - val_acc: 0.2399\n",
      "Epoch 19/100\n",
      "6078/6078 [==============================] - 3s 523us/step - loss: 16671.2174 - acc: 0.2085 - val_loss: 16654.5194 - val_acc: 0.2414\n",
      "Epoch 20/100\n",
      "6078/6078 [==============================] - 4s 588us/step - loss: 16640.7823 - acc: 0.2150 - val_loss: 16624.1219 - val_acc: 0.2414\n",
      "Epoch 21/100\n",
      "6078/6078 [==============================] - 4s 638us/step - loss: 16610.3697 - acc: 0.2113 - val_loss: 16593.7612 - val_acc: 0.2399\n",
      "Epoch 22/100\n",
      "6078/6078 [==============================] - 3s 485us/step - loss: 16579.8785 - acc: 0.2178 - val_loss: 16563.4631 - val_acc: 0.2461\n",
      "Epoch 23/100\n",
      "6078/6078 [==============================] - 3s 565us/step - loss: 16549.4156 - acc: 0.2188 - val_loss: 16533.1821 - val_acc: 0.2461\n",
      "Epoch 24/100\n",
      "6078/6078 [==============================] - 3s 537us/step - loss: 16519.0136 - acc: 0.2113 - val_loss: 16502.9309 - val_acc: 0.2508\n",
      "Epoch 25/100\n",
      "6078/6078 [==============================] - 3s 575us/step - loss: 16488.9109 - acc: 0.2106 - val_loss: 16472.7437 - val_acc: 0.2461\n",
      "Epoch 26/100\n",
      "6078/6078 [==============================] - 4s 609us/step - loss: 16458.6685 - acc: 0.2083 - val_loss: 16442.5977 - val_acc: 0.2508\n",
      "Epoch 27/100\n",
      "6078/6078 [==============================] - 4s 609us/step - loss: 16428.5371 - acc: 0.2085 - val_loss: 16412.4692 - val_acc: 0.2492\n",
      "Epoch 28/100\n",
      "6078/6078 [==============================] - 4s 611us/step - loss: 16398.1013 - acc: 0.2119 - val_loss: 16382.3853 - val_acc: 0.2492\n",
      "Epoch 29/100\n",
      "6078/6078 [==============================] - 3s 513us/step - loss: 16368.0042 - acc: 0.2167 - val_loss: 16352.3409 - val_acc: 0.2508\n",
      "Epoch 30/100\n",
      "6078/6078 [==============================] - 4s 618us/step - loss: 16338.2595 - acc: 0.2058 - val_loss: 16322.3388 - val_acc: 0.2461\n",
      "Epoch 31/100\n",
      "6078/6078 [==============================] - 3s 527us/step - loss: 16308.1522 - acc: 0.2267 - val_loss: 16292.3785 - val_acc: 0.2523\n",
      "Epoch 32/100\n",
      "6078/6078 [==============================] - 3s 542us/step - loss: 16277.7451 - acc: 0.2149 - val_loss: 16262.4362 - val_acc: 0.2555\n",
      "Epoch 33/100\n",
      "6078/6078 [==============================] - 3s 572us/step - loss: 16247.8090 - acc: 0.2109 - val_loss: 16232.5504 - val_acc: 0.2523\n",
      "Epoch 34/100\n",
      "6078/6078 [==============================] - 3s 512us/step - loss: 16218.0329 - acc: 0.2060 - val_loss: 16202.6794 - val_acc: 0.2508\n",
      "Epoch 35/100\n",
      "6078/6078 [==============================] - 4s 589us/step - loss: 16188.0421 - acc: 0.2085 - val_loss: 16172.8723 - val_acc: 0.2570\n",
      "Epoch 36/100\n",
      "6078/6078 [==============================] - 3s 566us/step - loss: 16158.3296 - acc: 0.2131 - val_loss: 16143.0891 - val_acc: 0.2492\n",
      "Epoch 37/100\n",
      "6078/6078 [==============================] - 4s 615us/step - loss: 16128.6430 - acc: 0.2042 - val_loss: 16113.3728 - val_acc: 0.2539\n",
      "Epoch 38/100\n",
      "6078/6078 [==============================] - 3s 509us/step - loss: 16098.4705 - acc: 0.2172 - val_loss: 16083.6547 - val_acc: 0.2539\n",
      "Epoch 39/100\n",
      "6078/6078 [==============================] - 4s 587us/step - loss: 16068.6842 - acc: 0.2122 - val_loss: 16053.9678 - val_acc: 0.2586\n",
      "Epoch 40/100\n",
      "6078/6078 [==============================] - 4s 585us/step - loss: 16039.0536 - acc: 0.2167 - val_loss: 16024.3455 - val_acc: 0.2523\n",
      "Epoch 41/100\n",
      "6078/6078 [==============================] - 4s 586us/step - loss: 16009.5071 - acc: 0.2150 - val_loss: 15994.7717 - val_acc: 0.2555\n",
      "Epoch 42/100\n",
      "6078/6078 [==============================] - 4s 588us/step - loss: 15979.8488 - acc: 0.2068 - val_loss: 15965.2179 - val_acc: 0.2539\n",
      "Epoch 43/100\n",
      "6078/6078 [==============================] - 4s 576us/step - loss: 15950.1082 - acc: 0.2170 - val_loss: 15935.7023 - val_acc: 0.2523\n",
      "Epoch 44/100\n",
      "6078/6078 [==============================] - 3s 507us/step - loss: 15920.6372 - acc: 0.2103 - val_loss: 15906.2104 - val_acc: 0.2555\n",
      "Epoch 45/100\n",
      "6078/6078 [==============================] - 4s 588us/step - loss: 15891.0272 - acc: 0.2170 - val_loss: 15876.7784 - val_acc: 0.2555\n",
      "Epoch 46/100\n",
      "6078/6078 [==============================] - 4s 611us/step - loss: 15861.4753 - acc: 0.2168 - val_loss: 15847.3571 - val_acc: 0.2539\n",
      "Epoch 47/100\n",
      "6078/6078 [==============================] - 4s 612us/step - loss: 15832.0813 - acc: 0.2014 - val_loss: 15817.9721 - val_acc: 0.2539\n",
      "Epoch 48/100\n",
      "6078/6078 [==============================] - 4s 609us/step - loss: 15802.5600 - acc: 0.2045 - val_loss: 15788.6047 - val_acc: 0.2523\n",
      "Epoch 49/100\n",
      "6078/6078 [==============================] - 4s 622us/step - loss: 15773.2342 - acc: 0.2157 - val_loss: 15759.2810 - val_acc: 0.2555\n",
      "Epoch 50/100\n",
      "6078/6078 [==============================] - 3s 519us/step - loss: 15743.8307 - acc: 0.2113 - val_loss: 15729.9977 - val_acc: 0.2555\n",
      "Epoch 51/100\n",
      "6078/6078 [==============================] - 4s 639us/step - loss: 15714.4621 - acc: 0.2127 - val_loss: 15700.7613 - val_acc: 0.2570\n",
      "Epoch 52/100\n",
      "6078/6078 [==============================] - 3s 520us/step - loss: 15685.1221 - acc: 0.2124 - val_loss: 15671.5429 - val_acc: 0.2523\n",
      "Epoch 53/100\n",
      "6078/6078 [==============================] - 4s 598us/step - loss: 15656.0329 - acc: 0.2187 - val_loss: 15642.3729 - val_acc: 0.2539\n",
      "Epoch 54/100\n",
      "6078/6078 [==============================] - 3s 554us/step - loss: 15626.8884 - acc: 0.2172 - val_loss: 15613.2217 - val_acc: 0.2555\n",
      "Epoch 55/100\n",
      "6078/6078 [==============================] - 3s 514us/step - loss: 15597.4843 - acc: 0.2172 - val_loss: 15584.1154 - val_acc: 0.2555\n",
      "Epoch 56/100\n",
      "6078/6078 [==============================] - 3s 524us/step - loss: 15568.3240 - acc: 0.2134 - val_loss: 15555.0080 - val_acc: 0.2492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "6078/6078 [==============================] - 3s 504us/step - loss: 15539.2863 - acc: 0.2119 - val_loss: 15525.9844 - val_acc: 0.2508\n",
      "Epoch 58/100\n",
      "6078/6078 [==============================] - 3s 501us/step - loss: 15510.2969 - acc: 0.2218 - val_loss: 15496.9826 - val_acc: 0.2508\n",
      "Epoch 59/100\n",
      "6078/6078 [==============================] - 3s 560us/step - loss: 15481.2285 - acc: 0.2170 - val_loss: 15468.0061 - val_acc: 0.2508\n",
      "Epoch 60/100\n",
      "6078/6078 [==============================] - 4s 591us/step - loss: 15452.2208 - acc: 0.2193 - val_loss: 15439.0381 - val_acc: 0.2477\n",
      "Epoch 61/100\n",
      "6078/6078 [==============================] - 4s 596us/step - loss: 15423.1802 - acc: 0.2073 - val_loss: 15410.1244 - val_acc: 0.2445\n",
      "Epoch 62/100\n",
      "6078/6078 [==============================] - 4s 592us/step - loss: 15394.0277 - acc: 0.2145 - val_loss: 15381.2209 - val_acc: 0.2445\n",
      "Epoch 63/100\n",
      "6078/6078 [==============================] - 4s 598us/step - loss: 15365.1774 - acc: 0.2155 - val_loss: 15352.3732 - val_acc: 0.2461\n",
      "Epoch 64/100\n",
      "6078/6078 [==============================] - 3s 489us/step - loss: 15336.1910 - acc: 0.2236 - val_loss: 15323.5490 - val_acc: 0.2477\n",
      "Epoch 65/100\n",
      "6078/6078 [==============================] - 3s 505us/step - loss: 15307.3584 - acc: 0.2134 - val_loss: 15294.7466 - val_acc: 0.2508\n",
      "Epoch 66/100\n",
      "6078/6078 [==============================] - 3s 524us/step - loss: 15278.5320 - acc: 0.2152 - val_loss: 15265.9802 - val_acc: 0.2508\n",
      "Epoch 67/100\n",
      "6078/6078 [==============================] - 3s 485us/step - loss: 15249.7693 - acc: 0.2150 - val_loss: 15237.2514 - val_acc: 0.2539\n",
      "Epoch 68/100\n",
      "6078/6078 [==============================] - 3s 566us/step - loss: 15220.9010 - acc: 0.2152 - val_loss: 15208.5308 - val_acc: 0.2492\n",
      "Epoch 69/100\n",
      "6078/6078 [==============================] - 3s 468us/step - loss: 15192.2585 - acc: 0.2124 - val_loss: 15179.8602 - val_acc: 0.2586\n",
      "Epoch 70/100\n",
      "6078/6078 [==============================] - 3s 494us/step - loss: 15163.6232 - acc: 0.2136 - val_loss: 15151.2098 - val_acc: 0.2477\n",
      "Epoch 71/100\n",
      "6078/6078 [==============================] - 3s 467us/step - loss: 15134.6791 - acc: 0.2081 - val_loss: 15122.6016 - val_acc: 0.2570\n",
      "Epoch 72/100\n",
      "6078/6078 [==============================] - 3s 572us/step - loss: 15106.1850 - acc: 0.2139 - val_loss: 15094.0239 - val_acc: 0.2523\n",
      "Epoch 73/100\n",
      "6078/6078 [==============================] - 3s 575us/step - loss: 15077.5805 - acc: 0.2093 - val_loss: 15065.4715 - val_acc: 0.2477\n",
      "Epoch 74/100\n",
      "6078/6078 [==============================] - 4s 589us/step - loss: 15048.8348 - acc: 0.2175 - val_loss: 15036.9676 - val_acc: 0.2523\n",
      "Epoch 75/100\n",
      "6078/6078 [==============================] - 4s 609us/step - loss: 15020.2321 - acc: 0.2119 - val_loss: 15008.4678 - val_acc: 0.2508\n",
      "Epoch 76/100\n",
      "6078/6078 [==============================] - 4s 610us/step - loss: 14991.8595 - acc: 0.2195 - val_loss: 14980.0025 - val_acc: 0.2539\n",
      "Epoch 77/100\n",
      "6078/6078 [==============================] - 3s 500us/step - loss: 14963.2489 - acc: 0.2192 - val_loss: 14951.5839 - val_acc: 0.2461\n",
      "Epoch 78/100\n",
      "6078/6078 [==============================] - 3s 549us/step - loss: 14934.7499 - acc: 0.2141 - val_loss: 14923.1866 - val_acc: 0.2523\n",
      "Epoch 79/100\n",
      "6078/6078 [==============================] - 4s 585us/step - loss: 14906.2703 - acc: 0.2203 - val_loss: 14894.8220 - val_acc: 0.2539\n",
      "Epoch 80/100\n",
      "6078/6078 [==============================] - 4s 597us/step - loss: 14877.9418 - acc: 0.2226 - val_loss: 14866.4841 - val_acc: 0.2445\n",
      "Epoch 81/100\n",
      "6078/6078 [==============================] - 4s 617us/step - loss: 14849.6324 - acc: 0.2113 - val_loss: 14838.1662 - val_acc: 0.2477\n",
      "Epoch 82/100\n",
      "6078/6078 [==============================] - 4s 606us/step - loss: 14821.2948 - acc: 0.2136 - val_loss: 14809.9032 - val_acc: 0.2570\n",
      "Epoch 83/100\n",
      "6078/6078 [==============================] - 4s 599us/step - loss: 14792.7542 - acc: 0.2187 - val_loss: 14781.6424 - val_acc: 0.2508\n",
      "Epoch 84/100\n",
      "6078/6078 [==============================] - 3s 535us/step - loss: 14764.8126 - acc: 0.2147 - val_loss: 14753.4429 - val_acc: 0.2570\n",
      "Epoch 85/100\n",
      "6078/6078 [==============================] - 3s 569us/step - loss: 14736.3365 - acc: 0.2269 - val_loss: 14725.2340 - val_acc: 0.2570\n",
      "Epoch 86/100\n",
      "6078/6078 [==============================] - 3s 566us/step - loss: 14708.1020 - acc: 0.2211 - val_loss: 14697.0852 - val_acc: 0.2570\n",
      "Epoch 87/100\n",
      "6078/6078 [==============================] - 3s 558us/step - loss: 14679.6305 - acc: 0.2173 - val_loss: 14668.9342 - val_acc: 0.2601\n",
      "Epoch 88/100\n",
      "6078/6078 [==============================] - 3s 559us/step - loss: 14651.7376 - acc: 0.2215 - val_loss: 14640.8487 - val_acc: 0.2570\n",
      "Epoch 89/100\n",
      "6078/6078 [==============================] - 3s 504us/step - loss: 14623.6805 - acc: 0.2167 - val_loss: 14612.7773 - val_acc: 0.2570\n",
      "Epoch 90/100\n",
      "6078/6078 [==============================] - 3s 568us/step - loss: 14595.6158 - acc: 0.2071 - val_loss: 14584.7584 - val_acc: 0.2601\n",
      "Epoch 91/100\n",
      "6078/6078 [==============================] - 4s 610us/step - loss: 14567.6004 - acc: 0.2124 - val_loss: 14556.7462 - val_acc: 0.2601\n",
      "Epoch 92/100\n",
      "6078/6078 [==============================] - 4s 606us/step - loss: 14539.2998 - acc: 0.2229 - val_loss: 14528.7568 - val_acc: 0.2617\n",
      "Epoch 93/100\n",
      "6078/6078 [==============================] - 4s 608us/step - loss: 14511.3141 - acc: 0.2226 - val_loss: 14500.7969 - val_acc: 0.2586\n",
      "Epoch 94/100\n",
      "6078/6078 [==============================] - 4s 608us/step - loss: 14483.3363 - acc: 0.2152 - val_loss: 14472.8743 - val_acc: 0.2617\n",
      "Epoch 95/100\n",
      "6078/6078 [==============================] - 3s 507us/step - loss: 14455.4449 - acc: 0.2177 - val_loss: 14444.9745 - val_acc: 0.2601\n",
      "Epoch 96/100\n",
      "6078/6078 [==============================] - 3s 499us/step - loss: 14427.3739 - acc: 0.2139 - val_loss: 14417.0827 - val_acc: 0.2601\n",
      "Epoch 97/100\n",
      "6078/6078 [==============================] - 3s 494us/step - loss: 14399.5587 - acc: 0.2267 - val_loss: 14389.2385 - val_acc: 0.2617\n",
      "Epoch 98/100\n",
      "6078/6078 [==============================] - 3s 505us/step - loss: 14371.7273 - acc: 0.2124 - val_loss: 14361.4043 - val_acc: 0.2570\n",
      "Epoch 99/100\n",
      "6078/6078 [==============================] - 3s 480us/step - loss: 14343.8428 - acc: 0.2122 - val_loss: 14333.6160 - val_acc: 0.2555\n",
      "Epoch 100/100\n",
      "6078/6078 [==============================] - 4s 579us/step - loss: 14315.9059 - acc: 0.2211 - val_loss: 14305.8326 - val_acc: 0.2539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2a4f8e10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, 5, padding='same', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01),\n",
    "                 input_shape=(272,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(32, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(64, 5, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.000001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

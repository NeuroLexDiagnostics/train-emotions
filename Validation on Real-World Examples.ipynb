{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucaslyon/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import librosa\n",
    "#import librosa.display\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.pyplot import specgram\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 217, 32)           192       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 217, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 108, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 108, 64)           10304     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 108, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 54, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 54, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               442496    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 453,895\n",
      "Trainable params: 453,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models  import load_model\n",
    "\n",
    "model = load_model(\"model2.h5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = np.expand_dims(image, <your desired dimension>)\n",
    "\n",
    "X, sample_rate = librosa.load(\"test_sample_d.wav\", res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "feature0 = mfccs\n",
    "\n",
    "X, sample_rate = librosa.load(\"test_sample_a.wav\", res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "feature1 = mfccs\n",
    "\n",
    "X, sample_rate = librosa.load(\"test_sample_s.wav\", res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "feature2 = mfccs\n",
    "\n",
    "X, sample_rate = librosa.load(\"test_sample_a_2.wav\", res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "feature3 = mfccs\n",
    "\n",
    "X, sample_rate = librosa.load(\"test_surprise.wav\", res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "feature4 = mfccs\n",
    "\n",
    "\n",
    "feature0 = np.pad(feature0, (0, 217-feature0.shape[0]), 'constant')\n",
    "feature0 = feature0.reshape(1, 217, 1)\n",
    "\n",
    "feature1 = np.pad(feature1, (0, 217-feature1.shape[0]), 'constant')\n",
    "feature1 = feature1.reshape(1, 217, 1)\n",
    "\n",
    "feature2 = np.pad(feature2, (0, 217-feature2.shape[0]), 'constant')\n",
    "feature2 = feature2.reshape(1, 217, 1)\n",
    "\n",
    "feature3 = np.pad(feature3, (0, 217-feature3.shape[0]), 'constant')\n",
    "feature3 = feature3.reshape(1, 217, 1)\n",
    "\n",
    "feature4 = np.pad(feature4, (0, 217-feature4.shape[0]), 'mean')\n",
    "feature4 = feature4.reshape(1,217,1)\n",
    "\n",
    "# feats = [feature0, feature1]\n",
    "\n",
    "prediction = model.predict(feature0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -36.119404\n",
       "1     -35.437660\n",
       "2     -35.342020\n",
       "3     -32.386917\n",
       "4     -30.536057\n",
       "5     -30.214967\n",
       "6     -27.654420\n",
       "7     -23.548437\n",
       "8     -24.878867\n",
       "9     -31.028803\n",
       "10    -33.188347\n",
       "11    -33.877209\n",
       "12    -33.297419\n",
       "13    -33.057617\n",
       "14    -32.846235\n",
       "15    -33.194697\n",
       "16    -34.317569\n",
       "17    -34.390428\n",
       "18    -33.190427\n",
       "19    -31.097272\n",
       "20    -29.841830\n",
       "21    -30.057643\n",
       "22    -32.577645\n",
       "23    -35.474520\n",
       "24    -36.859011\n",
       "25    -34.727209\n",
       "26    -35.814434\n",
       "27    -35.381270\n",
       "28    -33.907696\n",
       "29    -33.260987\n",
       "         ...    \n",
       "186     0.000000\n",
       "187     0.000000\n",
       "188     0.000000\n",
       "189     0.000000\n",
       "190     0.000000\n",
       "191     0.000000\n",
       "192     0.000000\n",
       "193     0.000000\n",
       "194     0.000000\n",
       "195     0.000000\n",
       "196     0.000000\n",
       "197     0.000000\n",
       "198     0.000000\n",
       "199     0.000000\n",
       "200     0.000000\n",
       "201     0.000000\n",
       "202     0.000000\n",
       "203     0.000000\n",
       "204     0.000000\n",
       "205     0.000000\n",
       "206     0.000000\n",
       "207     0.000000\n",
       "208     0.000000\n",
       "209     0.000000\n",
       "210     0.000000\n",
       "211     0.000000\n",
       "212     0.000000\n",
       "213     0.000000\n",
       "214     0.000000\n",
       "215     0.000000\n",
       "Name: 0, Length: 216, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotionsDf = pd.read_csv(\"9_24_2018_1_43_pm_emotions_and_speakers.csv\")\n",
    "emotionsDf = emotionsDf.drop('Unnamed: 0', 1)\n",
    "emotionsDf = emotionsDf.drop('0.1', 1)\n",
    "emotionsDf.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Importing Data for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Actor_01\n",
      "1 Actor_02\n",
      "2 Actor_03\n",
      "3 Actor_04\n",
      "4 Actor_05\n",
      "5 Actor_06\n",
      "6 Actor_07\n",
      "7 Actor_08\n",
      "8 Actor_09\n",
      "9 Actor_10\n",
      "10 Actor_11\n",
      "11 Actor_12\n",
      "12 Actor_13\n",
      "13 Actor_14\n",
      "14 Actor_15\n",
      "15 Actor_16\n",
      "16 Actor_17\n",
      "17 Actor_18\n",
      "18 Actor_19\n",
      "19 Actor_20\n",
      "20 Actor_21\n",
      "21 Actor_22\n",
      "22 Actor_23\n",
      "23 Actor_24\n"
     ]
    }
   ],
   "source": [
    "actorlist = os.listdir('Audio')\n",
    "mydir = 'Audio/'\n",
    "mylist = os.listdir(mydir)\n",
    "actorlist.sort()\n",
    "\n",
    "feeling_list=[]\n",
    "mydir = 'Audio/'\n",
    "for actor in actorlist:\n",
    "    try:\n",
    "        mylist = os.listdir(mydir+actor)\n",
    "        mylist.sort()\n",
    "        for item in mylist:\n",
    "            \n",
    "            #neutral\n",
    "            if item[6:-16]=='01' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(5)\n",
    "            elif item[6:-16]=='01' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(5)\n",
    "            #calm\n",
    "            if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(5)\n",
    "            elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(5)\n",
    "            #happy\n",
    "            elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(3)\n",
    "            elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(3)\n",
    "            #sad\n",
    "            elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(4)\n",
    "            elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(4)\n",
    "            #angry\n",
    "            elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(0)\n",
    "            elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(0)\n",
    "            #fear\n",
    "            elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(1)\n",
    "            elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(1)\n",
    "            #disgust\n",
    "            if item[6:-16]=='07' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(2)\n",
    "            elif item[6:-16]=='07' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(2)\n",
    "            #surprise\n",
    "            if item[6:-16]=='08' and int(item[18:-4])%2==0:\n",
    "                feeling_list.append(6)\n",
    "            elif item[6:-16]=='08' and int(item[18:-4])%2==1:\n",
    "                feeling_list.append(6)\n",
    "                \n",
    "    except: pass\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#labels = pd.DataFrame(feeling_list)\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for index,actor in enumerate(actorlist):\n",
    "    print(index, actor)\n",
    "    try:\n",
    "        recordinglist = os.listdir('Audio/'+actor)\n",
    "        for index, recording in enumerate(recordinglist):\n",
    "            X, sample_rate = librosa.load('Audio/'+actor+'/'+recording, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "            sample_rate = np.array(sample_rate)\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                                sr=sample_rate, \n",
    "                                                n_mfcc=13),\n",
    "                            axis=0)\n",
    "            feature = mfccs\n",
    "            df.loc[bookmark] = [feature]\n",
    "            bookmark=bookmark+1\n",
    "    except: pass\n",
    "\n",
    "actorDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "\n",
    "#labeledDf = pd.concat([featuresDf, labels], axis=1)\n",
    "\n",
    "#named = labeledDf.rename(index=str, columns={\"0\":\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/fear/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "    sample_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13),\n",
    "                    axis=0)\n",
    "    feature = mfccs\n",
    "    df.loc[bookmark] = [feature]\n",
    "    bookmark+=1\n",
    "fearDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#fearDf['label'] = \"fear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/angry/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "    sample_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13),\n",
    "                    axis=0)\n",
    "    feature = mfccs\n",
    "    df.loc[bookmark] = [feature]\n",
    "    bookmark+=1\n",
    "angryDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#angryDf['label'] = \"angry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/disgust/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "    sample_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13),\n",
    "                    axis=0)\n",
    "    feature = mfccs\n",
    "    df.loc[bookmark] = [feature]\n",
    "    bookmark+=1\n",
    "disgustDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#disgustDf['label'] = \"disgust\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/happy/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "    except:\n",
    "        pass\n",
    "happyDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#happyDf['label'] = \"happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/neutral/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "    except:\n",
    "        pass\n",
    "neutralDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#neutralDf['label'] = \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/surprise/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "    except:\n",
    "        pass\n",
    "surpriseDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#surpriseDf['label'] = \"surprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/sad/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "    except:\n",
    "        pass\n",
    "sadDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#sadDf['label'] = \"sad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/AudioData/DC/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "dc_feeling_list = []\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    \n",
    "    emotion = file[0]\n",
    "    \n",
    "    if emotion == 'a':\n",
    "        dc_feeling_list.append('angry')\n",
    "    elif emotion == 'd':\n",
    "        dc_feeling_list.append('disgust')\n",
    "    elif emotion == 'f':\n",
    "        dc_feeling_list.append('fear')\n",
    "    elif emotion == 'h':\n",
    "        dc_feeling_list.append('happy')\n",
    "    elif emotion == 'n':\n",
    "        dc_feeling_list.append('neutral')\n",
    "    elif emotion == 's' and file[1] == 'a':\n",
    "        dc_feeling_list.append('sad')\n",
    "    else: dc_feeling_list.append('surprise')\n",
    "    \n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "#labels = pd.DataFrame(dc_feeling_list)\n",
    "dcDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#dcDf = pd.concat([dcDf, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/AudioData/JE/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "je_feeling_list = []\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    \n",
    "    emotion = file[0]\n",
    "    \n",
    "    if emotion == 'a':\n",
    "        je_feeling_list.append('angry')\n",
    "    elif emotion == 'd':\n",
    "        je_feeling_list.append('disgust')\n",
    "    elif emotion == 'f':\n",
    "        je_feeling_list.append('fear')\n",
    "    elif emotion == 'h':\n",
    "        je_feeling_list.append('happy')\n",
    "    elif emotion == 'n':\n",
    "        je_feeling_list.append('neutral')\n",
    "    elif emotion == 's' and file[1] == 'a':\n",
    "        je_feeling_list.append('sad')\n",
    "    else: je_feeling_list.append('surprise')\n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "    except:\n",
    "        pass\n",
    "jeDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#labels = pd.DataFrame(je_feeling_list)\n",
    "#jeDf = pd.concat([jeDf, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/AudioData/JK/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "jk_feeling_list = []\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    \n",
    "    emotion = file[0]\n",
    "    \n",
    "    if emotion == 'a':\n",
    "        jk_feeling_list.append('angry')\n",
    "    elif emotion == 'd':\n",
    "        jk_feeling_list.append('disgust')\n",
    "    elif emotion == 'f':\n",
    "        jk_feeling_list.append('fear')\n",
    "    elif emotion == 'h':\n",
    "        jk_feeling_list.append('happy')\n",
    "    elif emotion == 'n':\n",
    "        jk_feeling_list.append('neutral')\n",
    "    elif emotion == 's' and file[1] == 'a':\n",
    "        jk_feeling_list.append('sad')\n",
    "    else: jk_feeling_list.append('surprise')\n",
    "    \n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "    except:\n",
    "        pass\n",
    "jkDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#labels = pd.DataFrame(jk_feeling_list)\n",
    "#jkDf = pd.concat([jkDf, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/AudioData/KL/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "kl_feeling_list = []\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    \n",
    "    emotion = file[0]\n",
    "    \n",
    "    if emotion == 'a':\n",
    "        kl_feeling_list.append('angry')\n",
    "    elif emotion == 'd':\n",
    "        kl_feeling_list.append('disgust')\n",
    "    elif emotion == 'f':\n",
    "        kl_feeling_list.append('fear')\n",
    "    elif emotion == 'h':\n",
    "        kl_feeling_list.append('happy')\n",
    "    elif emotion == 'n':\n",
    "        kl_feeling_list.append('neutral')\n",
    "    elif emotion == 's' and file[1] == 'a':\n",
    "        kl_feeling_list.append('sad')\n",
    "    else: kl_feeling_list.append('surprise')\n",
    "        \n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "    except:\n",
    "        pass\n",
    "klDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#labels = pd.DataFrame(kl_feeling_list)\n",
    "#klDf = pd.concat([klDf, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/AudioWAV/\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "wav_feeling_list = []\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    try:\n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "        \n",
    "        emotion = file[9:12] \n",
    "        \n",
    "        if emotion == \"ANG\":\n",
    "            wav_feeling_list.append(\"angry\")\n",
    "        elif emotion == \"DIS\":\n",
    "            wav_feeling_list.append(\"disgust\")\n",
    "        elif emotion == \"FEA\":\n",
    "            wav_feeling_list.append(\"fear\")\n",
    "        elif emotion == \"HAP\":\n",
    "            wav_feeling_list.append(\"happy\")\n",
    "        elif emotion == \"NEU\":\n",
    "            wav_feeling_list.append(\"neutral\")\n",
    "        elif emotion == \"SAD\":\n",
    "            wav_feeling_list.append(\"sad\")\n",
    "        else:\n",
    "            print(\"      \", emotion)\n",
    "    except:\n",
    "        pass\n",
    "wavDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#labelsDf = pd.DataFrame(feeling_list)\n",
    "#wavDf = pd.concat([wavDf, labelsDf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDir = \"./training_audio_files/emodb/wav\"\n",
    "trainFileList = os.listdir(trainDir)\n",
    "trainFileList.sort()\n",
    "emo_feeling_list = []\n",
    "\n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for file in trainFileList:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        X, sampling_rate = librosa.load(trainDir+file, duration=5.0)\n",
    "        sample_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark+=1\n",
    "        \n",
    "        emotion = file[5]\n",
    "        if emotion == 'A':\n",
    "            emo_feeling_list.append(\"fear\")\n",
    "        elif emotion == 'W':\n",
    "            emo_feeling_list.append(\"angry\")\n",
    "        elif emotion == 'L':\n",
    "            emo_feeling_list.append(\"bored\")\n",
    "        elif emotion == 'E':\n",
    "            emo_feeling_list.append(\"disgust\")\n",
    "        elif emotion == 'F':\n",
    "            emo_feeling_list.append(\"happy\")\n",
    "        elif emotion == 'T':\n",
    "            emo_feeling_list.append(\"sad\")\n",
    "        elif emotion == 'N':\n",
    "            emo_feeling_list.append(\"neutral\")\n",
    "        else:\n",
    "            print(\"error\", emotion)\n",
    "    except:\n",
    "        pass\n",
    "emoDf = pd.DataFrame(df['feature'].values.tolist())\n",
    "#labelDf = pd.DataFrame(feeling_list)\n",
    "#emoDf = pd.concat([emoDf, labelDf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat2 = pd.concat([dcDf,jeDf, jkDf, klDf])\n",
    "concatenatedDf = pd.concat([angryDf, fearDf, disgustDf, happyDf, sadDf, neutralDf, surpriseDf, concat2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a list of emotion labels for each audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_feeling_list = []\n",
    "for file in range(len(angryDf)):\n",
    "    compiled_feeling_list.append(0)\n",
    "for file in range(len(fearDf)):\n",
    "    compiled_feeling_list.append(1)\n",
    "for file in range(len(disgustDf)):\n",
    "    compiled_feeling_list.append(2)\n",
    "for file in range(len(happyDf)):\n",
    "    compiled_feeling_list.append(3)\n",
    "for file in range(len(sadDf)):\n",
    "    compiled_feeling_list.append(4)\n",
    "for file in range(len(neutralDf)):\n",
    "    compiled_feeling_list.append(5)\n",
    "for file in range(len(surpriseDf)):\n",
    "    compiled_feeling_list.append(6)\n",
    "\n",
    "dcjejkkl_feeling_list = dc_feeling_list + je_feeling_list + jk_feeling_list + kl_feeling_list\n",
    "for emo in range(len(dcjejkkl_feeling_list)):\n",
    "    if dcjejkkl_feeling_list[emo] == \"angry\":\n",
    "        compiled_feeling_list.append(0)\n",
    "    elif dcjejkkl_feeling_list[emo] == \"fear\":\n",
    "        compiled_feeling_list.append(1)\n",
    "    elif dcjejkkl_feeling_list[emo] == \"disgust\":\n",
    "        compiled_feeling_list.append(2)\n",
    "    elif dcjejkkl_feeling_list[emo] == \"happy\":\n",
    "        compiled_feeling_list.append(3)\n",
    "    elif dcjejkkl_feeling_list[emo] == \"sad\":\n",
    "        compiled_feeling_list.append(4)\n",
    "    elif dcjejkkl_feeling_list[emo] == \"neutral\":\n",
    "        compiled_feeling_list.append(5)\n",
    "    elif dcjejkkl_feeling_list[emo] == \"surprise\":\n",
    "        compiled_feeling_list.append(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Pandas DataFrame with emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feeling_list) == len(actorDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_feeling_list += feeling_list\n",
    "concatenatedDf = pd.concat([concatenatedDf, actorDf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelDf = pd.DataFrame(compiled_feeling_list)\n",
    "\n",
    "concatenatedDf = concatenatedDf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedDf = pd.concat([concatenatedDf, labelDf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedDf = concatenatedDf.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exported the labeled Pandas DataFrame for easier import of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedDf.to_csv(\"9_24_2018_3_02_pm_emotions_and_speakers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4068, 217)\n",
      "4068 train samples\n",
      "449 test samples\n"
     ]
    }
   ],
   "source": [
    "emotionsDf = pd.read_csv(\"9_24_2018_3_02_pm_emotions_and_speakers.csv\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "shuffledDf = shuffle(emotionsDf)\n",
    "\n",
    "divider = np.random.rand(len(shuffledDf)) < 0.9\n",
    "train = shuffledDf[divider]\n",
    "test = shuffledDf[~divider]\n",
    "\n",
    "trainfeatures = train.iloc[:, :-1]\n",
    "trainlabels = train.iloc[:, -1:]\n",
    "\n",
    "testfeatures = test.iloc[:, :-1]\n",
    "testlabels = test.iloc[:, -1:]\n",
    "\n",
    "x_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabels)\n",
    "x_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabels)\n",
    "\n",
    "#flattening the arrays\n",
    "y_train = np.hstack(y_train)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "#convert class vectors to binary class matrices.\n",
    "num_classes = 7\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train = np.expand_dims(x_train, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d5403d6f9aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, 5, padding='same',input_shape=(217,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(64, 5, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# model.add(Conv1D(128, 5, padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# model.add(Conv1D(256, 5, padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.Adadelta()\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4068 samples, validate on 449 samples\n",
      "Epoch 1/100\n",
      "4068/4068 [==============================] - 25s 6ms/step - loss: 11.0639 - acc: 0.2554 - val_loss: 7.9899 - val_acc: 0.4588\n",
      "Epoch 2/100\n",
      "4068/4068 [==============================] - 24s 6ms/step - loss: 5.1735 - acc: 0.3451 - val_loss: 1.5462 - val_acc: 0.4343\n",
      "Epoch 3/100\n",
      "4068/4068 [==============================] - 25s 6ms/step - loss: 1.4957 - acc: 0.4287 - val_loss: 1.1849 - val_acc: 0.5568\n",
      "Epoch 4/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 1.3494 - acc: 0.4749 - val_loss: 1.1800 - val_acc: 0.5412\n",
      "Epoch 5/100\n",
      "4068/4068 [==============================] - 22s 5ms/step - loss: 1.2712 - acc: 0.5081 - val_loss: 1.1516 - val_acc: 0.5412\n",
      "Epoch 6/100\n",
      "4068/4068 [==============================] - 26s 6ms/step - loss: 1.1960 - acc: 0.5315 - val_loss: 1.0650 - val_acc: 0.6013\n",
      "Epoch 7/100\n",
      "4068/4068 [==============================] - 26s 6ms/step - loss: 1.1470 - acc: 0.5457 - val_loss: 1.0032 - val_acc: 0.5924\n",
      "Epoch 8/100\n",
      "4068/4068 [==============================] - 25s 6ms/step - loss: 1.1052 - acc: 0.5656 - val_loss: 1.0427 - val_acc: 0.5523\n",
      "Epoch 9/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 1.0858 - acc: 0.5632 - val_loss: 0.9083 - val_acc: 0.6548\n",
      "Epoch 10/100\n",
      "4068/4068 [==============================] - 26s 7ms/step - loss: 1.0674 - acc: 0.5765 - val_loss: 0.8773 - val_acc: 0.6659\n",
      "Epoch 11/100\n",
      "4068/4068 [==============================] - 26s 7ms/step - loss: 1.0428 - acc: 0.5811 - val_loss: 0.9186 - val_acc: 0.6637\n",
      "Epoch 12/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 1.0235 - acc: 0.5885 - val_loss: 1.0572 - val_acc: 0.5501\n",
      "Epoch 13/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 1.0152 - acc: 0.5993 - val_loss: 1.1238 - val_acc: 0.5768\n",
      "Epoch 14/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 0.9966 - acc: 0.6042 - val_loss: 0.8506 - val_acc: 0.6659\n",
      "Epoch 15/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 0.9934 - acc: 0.6148 - val_loss: 0.9447 - val_acc: 0.6080\n",
      "Epoch 16/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 0.9717 - acc: 0.6177 - val_loss: 0.8425 - val_acc: 0.6570\n",
      "Epoch 17/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 0.9534 - acc: 0.6229 - val_loss: 0.8140 - val_acc: 0.6949\n",
      "Epoch 18/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 0.9380 - acc: 0.6323 - val_loss: 0.8017 - val_acc: 0.6949\n",
      "Epoch 19/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 0.9263 - acc: 0.6342 - val_loss: 0.8090 - val_acc: 0.6904\n",
      "Epoch 20/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.9194 - acc: 0.6377 - val_loss: 0.8202 - val_acc: 0.6904\n",
      "Epoch 21/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.9185 - acc: 0.6433 - val_loss: 0.7587 - val_acc: 0.6993\n",
      "Epoch 22/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.9051 - acc: 0.6465 - val_loss: 0.8773 - val_acc: 0.6414\n",
      "Epoch 23/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.9001 - acc: 0.6426 - val_loss: 0.8514 - val_acc: 0.6548\n",
      "Epoch 24/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8773 - acc: 0.6524 - val_loss: 1.0014 - val_acc: 0.6570\n",
      "Epoch 25/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8832 - acc: 0.6573 - val_loss: 0.7434 - val_acc: 0.7016\n",
      "Epoch 26/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8634 - acc: 0.6586 - val_loss: 0.7788 - val_acc: 0.6971\n",
      "Epoch 27/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8712 - acc: 0.6586 - val_loss: 0.7504 - val_acc: 0.7016\n",
      "Epoch 28/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8696 - acc: 0.6566 - val_loss: 0.7520 - val_acc: 0.7016\n",
      "Epoch 29/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8563 - acc: 0.6583 - val_loss: 0.7325 - val_acc: 0.7082\n",
      "Epoch 30/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8511 - acc: 0.6630 - val_loss: 0.7546 - val_acc: 0.6949\n",
      "Epoch 31/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8656 - acc: 0.6595 - val_loss: 0.7446 - val_acc: 0.7105\n",
      "Epoch 32/100\n",
      "4068/4068 [==============================] - 29s 7ms/step - loss: 0.8540 - acc: 0.6662 - val_loss: 0.7308 - val_acc: 0.7060\n",
      "Epoch 33/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8588 - acc: 0.6632 - val_loss: 0.8746 - val_acc: 0.6570\n",
      "Epoch 34/100\n",
      "4068/4068 [==============================] - 29s 7ms/step - loss: 0.8431 - acc: 0.6647 - val_loss: 0.7344 - val_acc: 0.7060\n",
      "Epoch 35/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8290 - acc: 0.6664 - val_loss: 0.7456 - val_acc: 0.7016\n",
      "Epoch 36/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8561 - acc: 0.6652 - val_loss: 0.7370 - val_acc: 0.7060\n",
      "Epoch 37/100\n",
      "4068/4068 [==============================] - 29s 7ms/step - loss: 0.8235 - acc: 0.6701 - val_loss: 0.7298 - val_acc: 0.7016\n",
      "Epoch 38/100\n",
      "4068/4068 [==============================] - 25s 6ms/step - loss: 0.8359 - acc: 0.6716 - val_loss: 0.7401 - val_acc: 0.7082\n",
      "Epoch 39/100\n",
      "4068/4068 [==============================] - 21s 5ms/step - loss: 0.8298 - acc: 0.6708 - val_loss: 0.7345 - val_acc: 0.7038\n",
      "Epoch 40/100\n",
      "4068/4068 [==============================] - 22s 6ms/step - loss: 0.8286 - acc: 0.6696 - val_loss: 0.7273 - val_acc: 0.7060\n",
      "Epoch 41/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8226 - acc: 0.6699 - val_loss: 0.7312 - val_acc: 0.7016\n",
      "Epoch 42/100\n",
      "4068/4068 [==============================] - 25s 6ms/step - loss: 0.8213 - acc: 0.6713 - val_loss: 0.7481 - val_acc: 0.7038\n",
      "Epoch 43/100\n",
      "4068/4068 [==============================] - 23s 6ms/step - loss: 0.8160 - acc: 0.6723 - val_loss: 0.7392 - val_acc: 0.7016\n",
      "Epoch 44/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8122 - acc: 0.6748 - val_loss: 0.7331 - val_acc: 0.7082\n",
      "Epoch 45/100\n",
      "4068/4068 [==============================] - 28s 7ms/step - loss: 0.8290 - acc: 0.6718 - val_loss: 0.7188 - val_acc: 0.7105\n",
      "Epoch 46/100\n",
      "4068/4068 [==============================] - 22s 6ms/step - loss: 0.8102 - acc: 0.6716 - val_loss: 0.7899 - val_acc: 0.6860\n",
      "Epoch 47/100\n",
      "4068/4068 [==============================] - 27s 7ms/step - loss: 0.8083 - acc: 0.6743 - val_loss: 0.7327 - val_acc: 0.7082\n",
      "Epoch 48/100\n",
      "4068/4068 [==============================] - 38s 9ms/step - loss: 0.8132 - acc: 0.6723 - val_loss: 0.7250 - val_acc: 0.7060\n",
      "Epoch 49/100\n",
      "4068/4068 [==============================] - 39s 10ms/step - loss: 0.8036 - acc: 0.6807 - val_loss: 0.7229 - val_acc: 0.7127\n",
      "Epoch 50/100\n",
      "4068/4068 [==============================] - 38s 9ms/step - loss: 0.8077 - acc: 0.6755 - val_loss: 0.7687 - val_acc: 0.6971\n",
      "Epoch 51/100\n",
      "4068/4068 [==============================] - 36s 9ms/step - loss: 0.8225 - acc: 0.6733 - val_loss: 0.7260 - val_acc: 0.7060\n",
      "Epoch 52/100\n",
      "4068/4068 [==============================] - 34s 8ms/step - loss: 0.8045 - acc: 0.6822 - val_loss: 0.7224 - val_acc: 0.7082\n",
      "Epoch 53/100\n",
      "4068/4068 [==============================] - 33s 8ms/step - loss: 0.7921 - acc: 0.6785 - val_loss: 0.8007 - val_acc: 0.6860\n",
      "Epoch 54/100\n",
      "4068/4068 [==============================] - 32s 8ms/step - loss: 0.7932 - acc: 0.6799 - val_loss: 0.7261 - val_acc: 0.7082\n",
      "Epoch 55/100\n",
      "4068/4068 [==============================] - 36s 9ms/step - loss: 0.8143 - acc: 0.6758 - val_loss: 0.7616 - val_acc: 0.6993\n",
      "Epoch 56/100\n",
      "4068/4068 [==============================] - 52s 13ms/step - loss: 0.8079 - acc: 0.6790 - val_loss: 0.7300 - val_acc: 0.7082\n",
      "Epoch 57/100\n",
      " 160/4068 [>.............................] - ETA: 1:15 - loss: 0.9388 - acc: 0.6438"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b405b5b29d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "cnnhistory = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnnhistory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2c5a53dfaa22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnnhistory' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#sigmoid\n",
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain model2.h (9_24_2018_1_43_pm_emotions_and_speakers.csv)  with J. Schwoebel's JSON data\n",
    "### Completely rebuild model with J. S. JSON data and  9_24_2018_3_03_pm_emotions_and_speakers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using code from https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucaslyon/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/lucaslyon/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import librosa\n",
    "#import librosa.display\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#from matplotlib.pyplot import specgram\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataBuilder():\n",
    "    \n",
    "    #pulls in JSON files Jim Schwoebel provided of features of audio data\n",
    "    #returns list of features for each emotion\n",
    "    #return lists: angryData, disgustData, fearData, happyData, neutralData, sadData, surpriseData\n",
    "    \n",
    "    angryJSONFiles = os.listdir('emotion_train_set/angry')\n",
    "    angryJSONFiles.sort()\n",
    "    angryData=[]\n",
    "    for file in angryJSONFiles:\n",
    "        try:\n",
    "            with open('emotion_train_set/angry/'+file) as json_data:\n",
    "                d = json.load(json_data)\n",
    "                angryData.append(d['features'])\n",
    "        except: pass\n",
    "    disgustJSONFiles = os.listdir('emotion_train_set/disgust')\n",
    "    disgustJSONFiles.sort()\n",
    "    disgustData=[]\n",
    "    for file in disgustJSONFiles:\n",
    "        try:\n",
    "            with open('emotion_train_set/disgust/'+file) as json_data:\n",
    "                d = json.load(json_data)\n",
    "                disgustData.append(d['features'])\n",
    "        except: pass\n",
    "\n",
    "    fearJSONFiles = os.listdir('emotion_train_set/fear')\n",
    "    fearJSONFiles.sort()\n",
    "    fearData=[]\n",
    "    for file in fearJSONFiles:\n",
    "        try:\n",
    "            with open('emotion_train_set/fear/'+file) as json_data:\n",
    "                d = json.load(json_data)\n",
    "                fearData.append(d['features'])\n",
    "        except: pass\n",
    "\n",
    "    happyJSONFiles = os.listdir('emotion_train_set/happy')\n",
    "    happyJSONFiles.sort()\n",
    "    happyData=[]\n",
    "    for file in happyJSONFiles:\n",
    "        try:\n",
    "            with open('emotion_train_set/happy/'+file) as json_data:\n",
    "                d = json.load(json_data)\n",
    "                happyData.append(d['features'])\n",
    "        except: pass\n",
    "\n",
    "    neutralJSONFiles = os.listdir('emotion_train_set/neutral')\n",
    "    neutralJSONFiles.sort()\n",
    "    neutralData=[]\n",
    "    for file in neutralJSONFiles:\n",
    "        try:\n",
    "            with open('emotion_train_set/neutral/'+file) as json_data:\n",
    "                d = json.load(json_data)\n",
    "                neutralData.append(d['features'])\n",
    "        except: pass\n",
    "\n",
    "    sadJSONFiles = os.listdir('emotion_train_set/sad')\n",
    "    sadJSONFiles.sort()\n",
    "    sadData=[]\n",
    "    for file in sadJSONFiles:\n",
    "        try:\n",
    "            with open('emotion_train_set/sad/'+file) as json_data:\n",
    "                d = json.load(json_data)\n",
    "                sadData.append(d['features'])\n",
    "        except: pass\n",
    "\n",
    "    surpriseJSONFiles = os.listdir('emotion_train_set/surprise')\n",
    "    surpriseJSONFiles.sort()\n",
    "    surpriseData=[]\n",
    "    for file in surpriseJSONFiles:\n",
    "        try:\n",
    "            with open('emotion_train_set/surprise/'+file) as json_data:\n",
    "                d = json.load(json_data)\n",
    "                surpriseData.append(d['features'])\n",
    "        except: pass\n",
    "    \n",
    "    return angryData, disgustData, fearData, happyData, neutralData, sadData, surpriseData\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildDataFrame(angryData, disgustData, fearData, happyData, neutralData, sadData, surpriseData):\n",
    "\n",
    "    #takes in lists of features\n",
    "    #returns a fully populated and labeled emotion dataframe\n",
    "    \n",
    "    angryDf = pd.DataFrame(angryData)\n",
    "    disgustDf = pd.DataFrame(disgustData)\n",
    "    fearDf = pd.DataFrame(fearData)\n",
    "    happyDf = pd.DataFrame(happyData)\n",
    "    neutralDf = pd.DataFrame(neutralData)\n",
    "    sadDf = pd.DataFrame(sadData)\n",
    "    surpriseDf = pd.DataFrame(surpriseData)\n",
    "\n",
    "    angryDf['label'] = 0 #'angry'\n",
    "    disgustDf['label'] = 1 #'disgust'\n",
    "    fearDf['label'] = 2 #'fear'\n",
    "    happyDf['label'] = 3 #'happy'\n",
    "    neutralDf['label'] = 4 #'neutral'\n",
    "    sadDf['label'] = 5 #'sad'\n",
    "    surpriseDf['label'] = 6 #'surprise'\n",
    "\n",
    "    emotionsDf = pd.concat([angryDf, disgustDf, fearDf, happyDf, neutralDf, sadDf, surpriseDf])\n",
    "    \n",
    "    return emotionsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#angry, disgust, fear, happy, neutral, sad, surprise = dataBuilder()\n",
    "#emotionsDf = buildDataFrame(angry, disgust, fear, happy, neutral, sad, surprise)\n",
    "#emotionsDf.to_csv('emotionsDfV1.csv', index=False)\n",
    "emotionsDf = pd.read_csv('emotionsDfV1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (6392, 272)\n",
      "6392 train samples\n",
      "328 test samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "shuffledDf = shuffle(emotionsDf)\n",
    "\n",
    "divider = np.random.rand(len(shuffledDf)) < 0.95\n",
    "train = shuffledDf[divider]\n",
    "test = shuffledDf[~divider]\n",
    "\n",
    "trainfeatures = train.iloc[:, :-1]\n",
    "trainlabels = train.iloc[:, -1:]\n",
    "\n",
    "testfeatures = test.iloc[:, :-1]\n",
    "testlabels = test.iloc[:, -1:]\n",
    "\n",
    "x_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabels)\n",
    "x_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabels)\n",
    "\n",
    "#flattening the arrays\n",
    "y_train = np.hstack(y_train)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "#convert class vectors to binary class matrices.\n",
    "num_classes = 7\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train = np.expand_dims(x_train, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5674, 272)\n",
      "5674 train samples\n",
      "1046 test samples\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, 5, padding='same',\n",
    "                 input_shape=(272,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(32, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(64, 5, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5660 samples, validate on 1060 samples\n",
      "Epoch 1/100\n",
      "5660/5660 [==============================] - 7s 1ms/step - loss: 3.2815 - acc: 0.2182 - val_loss: 1.7941 - val_acc: 0.2632\n",
      "Epoch 2/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.7974 - acc: 0.2360 - val_loss: 1.7621 - val_acc: 0.2802\n",
      "Epoch 3/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.7610 - acc: 0.2527 - val_loss: 1.7504 - val_acc: 0.2802\n",
      "Epoch 4/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.7350 - acc: 0.2509 - val_loss: 1.7276 - val_acc: 0.2915\n",
      "Epoch 5/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.7198 - acc: 0.2675 - val_loss: 1.7611 - val_acc: 0.2783\n",
      "Epoch 6/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.7056 - acc: 0.2689 - val_loss: 1.7439 - val_acc: 0.2802\n",
      "Epoch 7/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6973 - acc: 0.2684 - val_loss: 1.7416 - val_acc: 0.3104\n",
      "Epoch 8/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6898 - acc: 0.2763 - val_loss: 1.7362 - val_acc: 0.2981\n",
      "Epoch 9/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6820 - acc: 0.2890 - val_loss: 1.7186 - val_acc: 0.3123\n",
      "Epoch 10/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6719 - acc: 0.2903 - val_loss: 1.7134 - val_acc: 0.3075\n",
      "Epoch 11/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6703 - acc: 0.2896 - val_loss: 1.7059 - val_acc: 0.3123\n",
      "Epoch 12/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6611 - acc: 0.3016 - val_loss: 1.6853 - val_acc: 0.3160\n",
      "Epoch 13/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6593 - acc: 0.3025 - val_loss: 1.6770 - val_acc: 0.3274\n",
      "Epoch 14/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6565 - acc: 0.3000 - val_loss: 1.6865 - val_acc: 0.3170\n",
      "Epoch 15/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6507 - acc: 0.3065 - val_loss: 1.6771 - val_acc: 0.3264\n",
      "Epoch 16/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6434 - acc: 0.3072 - val_loss: 1.6627 - val_acc: 0.3217\n",
      "Epoch 17/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6389 - acc: 0.3161 - val_loss: 1.6737 - val_acc: 0.3292\n",
      "Epoch 18/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6369 - acc: 0.3083 - val_loss: 1.6732 - val_acc: 0.3311\n",
      "Epoch 19/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6367 - acc: 0.3208 - val_loss: 1.6573 - val_acc: 0.3349\n",
      "Epoch 20/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6334 - acc: 0.3231 - val_loss: 1.6554 - val_acc: 0.3396\n",
      "Epoch 21/100\n",
      "5660/5660 [==============================] - 7s 1ms/step - loss: 1.6296 - acc: 0.3164 - val_loss: 1.6625 - val_acc: 0.3453\n",
      "Epoch 22/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6286 - acc: 0.3246 - val_loss: 1.6564 - val_acc: 0.3292\n",
      "Epoch 23/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6230 - acc: 0.3277 - val_loss: 1.6638 - val_acc: 0.3415\n",
      "Epoch 24/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6245 - acc: 0.3306 - val_loss: 1.6511 - val_acc: 0.3406\n",
      "Epoch 25/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6207 - acc: 0.3239 - val_loss: 1.6428 - val_acc: 0.3425\n",
      "Epoch 26/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6121 - acc: 0.3309 - val_loss: 1.6499 - val_acc: 0.3330\n",
      "Epoch 27/100\n",
      "5660/5660 [==============================] - 7s 1ms/step - loss: 1.6135 - acc: 0.3403 - val_loss: 1.6556 - val_acc: 0.3406\n",
      "Epoch 28/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6135 - acc: 0.3279 - val_loss: 1.6425 - val_acc: 0.3472\n",
      "Epoch 29/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6091 - acc: 0.3304 - val_loss: 1.6413 - val_acc: 0.3406\n",
      "Epoch 30/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6057 - acc: 0.3380 - val_loss: 1.6429 - val_acc: 0.3311\n",
      "Epoch 31/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5981 - acc: 0.3392 - val_loss: 1.6488 - val_acc: 0.3245\n",
      "Epoch 32/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.6055 - acc: 0.3348 - val_loss: 1.6458 - val_acc: 0.3368\n",
      "Epoch 33/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5947 - acc: 0.3426 - val_loss: 1.6388 - val_acc: 0.3302\n",
      "Epoch 34/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5993 - acc: 0.3392 - val_loss: 1.6456 - val_acc: 0.3415\n",
      "Epoch 35/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5962 - acc: 0.3428 - val_loss: 1.6430 - val_acc: 0.3425\n",
      "Epoch 36/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5958 - acc: 0.3394 - val_loss: 1.6430 - val_acc: 0.3358\n",
      "Epoch 37/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5930 - acc: 0.3428 - val_loss: 1.6398 - val_acc: 0.3377\n",
      "Epoch 38/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5873 - acc: 0.3507 - val_loss: 1.6399 - val_acc: 0.3264\n",
      "Epoch 39/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5923 - acc: 0.3518 - val_loss: 1.6415 - val_acc: 0.3349\n",
      "Epoch 40/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5874 - acc: 0.3495 - val_loss: 1.6397 - val_acc: 0.3368\n",
      "Epoch 41/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5839 - acc: 0.3516 - val_loss: 1.6435 - val_acc: 0.3311\n",
      "Epoch 42/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5790 - acc: 0.3523 - val_loss: 1.6421 - val_acc: 0.3311\n",
      "Epoch 43/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5769 - acc: 0.3516 - val_loss: 1.6491 - val_acc: 0.3274\n",
      "Epoch 44/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5761 - acc: 0.3528 - val_loss: 1.6485 - val_acc: 0.3245\n",
      "Epoch 45/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5749 - acc: 0.3565 - val_loss: 1.6537 - val_acc: 0.3245\n",
      "Epoch 46/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5740 - acc: 0.3560 - val_loss: 1.6503 - val_acc: 0.3340\n",
      "Epoch 47/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5755 - acc: 0.3527 - val_loss: 1.6465 - val_acc: 0.3302\n",
      "Epoch 48/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5657 - acc: 0.3578 - val_loss: 1.6506 - val_acc: 0.3104\n",
      "Epoch 49/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5713 - acc: 0.3523 - val_loss: 1.6485 - val_acc: 0.3142\n",
      "Epoch 50/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5638 - acc: 0.3624 - val_loss: 1.6515 - val_acc: 0.3208\n",
      "Epoch 51/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5725 - acc: 0.3519 - val_loss: 1.6622 - val_acc: 0.3009\n",
      "Epoch 52/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5633 - acc: 0.3608 - val_loss: 1.6551 - val_acc: 0.3132\n",
      "Epoch 53/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5577 - acc: 0.3650 - val_loss: 1.6558 - val_acc: 0.3179\n",
      "Epoch 54/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5627 - acc: 0.3636 - val_loss: 1.6510 - val_acc: 0.3151\n",
      "Epoch 55/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5563 - acc: 0.3684 - val_loss: 1.6500 - val_acc: 0.3255\n",
      "Epoch 56/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5514 - acc: 0.3749 - val_loss: 1.6502 - val_acc: 0.3274\n",
      "Epoch 57/100\n",
      "5660/5660 [==============================] - 7s 1ms/step - loss: 1.5577 - acc: 0.3670 - val_loss: 1.6499 - val_acc: 0.3189\n",
      "Epoch 58/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5478 - acc: 0.3622 - val_loss: 1.6513 - val_acc: 0.3160\n",
      "Epoch 59/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5472 - acc: 0.3739 - val_loss: 1.6560 - val_acc: 0.3283\n",
      "Epoch 60/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5412 - acc: 0.3746 - val_loss: 1.6590 - val_acc: 0.3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5387 - acc: 0.3723 - val_loss: 1.6525 - val_acc: 0.3123\n",
      "Epoch 62/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5398 - acc: 0.3758 - val_loss: 1.6499 - val_acc: 0.3217\n",
      "Epoch 63/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5345 - acc: 0.3714 - val_loss: 1.6516 - val_acc: 0.3094\n",
      "Epoch 64/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5392 - acc: 0.3760 - val_loss: 1.6541 - val_acc: 0.3085\n",
      "Epoch 65/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5398 - acc: 0.3742 - val_loss: 1.6542 - val_acc: 0.3000\n",
      "Epoch 66/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5324 - acc: 0.3818 - val_loss: 1.6534 - val_acc: 0.3113\n",
      "Epoch 67/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5302 - acc: 0.3820 - val_loss: 1.6505 - val_acc: 0.3075\n",
      "Epoch 68/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5372 - acc: 0.3823 - val_loss: 1.6547 - val_acc: 0.3057\n",
      "Epoch 69/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5260 - acc: 0.3889 - val_loss: 1.6548 - val_acc: 0.3198\n",
      "Epoch 70/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5264 - acc: 0.3813 - val_loss: 1.6598 - val_acc: 0.2991\n",
      "Epoch 71/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5254 - acc: 0.3797 - val_loss: 1.6578 - val_acc: 0.3075\n",
      "Epoch 72/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5242 - acc: 0.3809 - val_loss: 1.6550 - val_acc: 0.3142\n",
      "Epoch 73/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5223 - acc: 0.3867 - val_loss: 1.6581 - val_acc: 0.3132\n",
      "Epoch 74/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5203 - acc: 0.3912 - val_loss: 1.6626 - val_acc: 0.3094\n",
      "Epoch 75/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5206 - acc: 0.3882 - val_loss: 1.6569 - val_acc: 0.3028\n",
      "Epoch 76/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5126 - acc: 0.3869 - val_loss: 1.6654 - val_acc: 0.2934\n",
      "Epoch 77/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5163 - acc: 0.3885 - val_loss: 1.6705 - val_acc: 0.3094\n",
      "Epoch 78/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5096 - acc: 0.3924 - val_loss: 1.6600 - val_acc: 0.3151\n",
      "Epoch 79/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5068 - acc: 0.3922 - val_loss: 1.6587 - val_acc: 0.3000\n",
      "Epoch 80/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4991 - acc: 0.3972 - val_loss: 1.6616 - val_acc: 0.3028\n",
      "Epoch 81/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5035 - acc: 0.3942 - val_loss: 1.6609 - val_acc: 0.3057\n",
      "Epoch 82/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5087 - acc: 0.3892 - val_loss: 1.6642 - val_acc: 0.3047\n",
      "Epoch 83/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4976 - acc: 0.3885 - val_loss: 1.6662 - val_acc: 0.2925\n",
      "Epoch 84/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5079 - acc: 0.3899 - val_loss: 1.6707 - val_acc: 0.3038\n",
      "Epoch 85/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.5016 - acc: 0.4028 - val_loss: 1.6700 - val_acc: 0.3123\n",
      "Epoch 86/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4993 - acc: 0.4000 - val_loss: 1.6697 - val_acc: 0.2981\n",
      "Epoch 87/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4971 - acc: 0.4030 - val_loss: 1.6619 - val_acc: 0.2972\n",
      "Epoch 88/100\n",
      "5660/5660 [==============================] - 7s 1ms/step - loss: 1.4951 - acc: 0.3970 - val_loss: 1.6663 - val_acc: 0.3057\n",
      "Epoch 89/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4930 - acc: 0.4076 - val_loss: 1.6705 - val_acc: 0.3038\n",
      "Epoch 90/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4879 - acc: 0.3945 - val_loss: 1.6655 - val_acc: 0.3028\n",
      "Epoch 91/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4761 - acc: 0.4125 - val_loss: 1.6686 - val_acc: 0.2943\n",
      "Epoch 92/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4908 - acc: 0.4094 - val_loss: 1.6701 - val_acc: 0.2849\n",
      "Epoch 93/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4830 - acc: 0.4099 - val_loss: 1.6784 - val_acc: 0.2991\n",
      "Epoch 94/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4790 - acc: 0.4171 - val_loss: 1.6688 - val_acc: 0.2943\n",
      "Epoch 95/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4899 - acc: 0.4012 - val_loss: 1.6728 - val_acc: 0.2934\n",
      "Epoch 96/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4743 - acc: 0.4090 - val_loss: 1.6672 - val_acc: 0.3019\n",
      "Epoch 97/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4655 - acc: 0.4134 - val_loss: 1.6778 - val_acc: 0.3019\n",
      "Epoch 98/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4768 - acc: 0.4140 - val_loss: 1.6720 - val_acc: 0.3151\n",
      "Epoch 99/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4638 - acc: 0.4148 - val_loss: 1.6712 - val_acc: 0.3094\n",
      "Epoch 100/100\n",
      "5660/5660 [==============================] - 6s 1ms/step - loss: 1.4605 - acc: 0.4186 - val_loss: 1.6812 - val_acc: 0.3057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1cef49b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adjusting dropout layer from 0.25 to 0.75\n",
    "#remving some massive 500 neuron layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 5, padding='same',\n",
    "                 input_shape=(272,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(32, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(64, 5, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5674 samples, validate on 1046 samples\n",
      "Epoch 1/10\n",
      "5674/5674 [==============================] - 4s 759us/step - loss: 6.2644 - acc: 0.2076 - val_loss: 1.8210 - val_acc: 0.2151\n",
      "Epoch 2/10\n",
      "5674/5674 [==============================] - 4s 705us/step - loss: 2.3633 - acc: 0.2180 - val_loss: 1.7860 - val_acc: 0.2543\n",
      "Epoch 3/10\n",
      "5674/5674 [==============================] - 4s 690us/step - loss: 1.7966 - acc: 0.2462 - val_loss: 1.7729 - val_acc: 0.2753\n",
      "Epoch 4/10\n",
      "5674/5674 [==============================] - 4s 695us/step - loss: 1.7358 - acc: 0.2522 - val_loss: 1.7408 - val_acc: 0.2782\n",
      "Epoch 5/10\n",
      "5674/5674 [==============================] - 4s 699us/step - loss: 1.7266 - acc: 0.2543 - val_loss: 1.7277 - val_acc: 0.2830\n",
      "Epoch 6/10\n",
      "5674/5674 [==============================] - 4s 719us/step - loss: 1.7069 - acc: 0.2651 - val_loss: 1.7400 - val_acc: 0.2925\n",
      "Epoch 7/10\n",
      "5674/5674 [==============================] - 4s 714us/step - loss: 1.7069 - acc: 0.2688 - val_loss: 1.7231 - val_acc: 0.3031\n",
      "Epoch 8/10\n",
      "5674/5674 [==============================] - 4s 708us/step - loss: 1.6967 - acc: 0.2742 - val_loss: 1.7079 - val_acc: 0.3059\n",
      "Epoch 9/10\n",
      "5674/5674 [==============================] - 4s 704us/step - loss: 1.6895 - acc: 0.2804 - val_loss: 1.7053 - val_acc: 0.3002\n",
      "Epoch 10/10\n",
      "5674/5674 [==============================] - 4s 716us/step - loss: 1.6921 - acc: 0.2742 - val_loss: 1.7167 - val_acc: 0.2964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c22254a20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1140pt\" viewBox=\"0.00 0.00 171.00 1140.00\" width=\"171pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1136)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1136 167,-1136 167,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 120831999504 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>120831999504</title>\n",
       "<polygon fill=\"none\" points=\"32,-1022.5 32,-1058.5 131,-1058.5 131,-1022.5 32,-1022.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-1036.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 120831999952 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>120831999952</title>\n",
       "<polygon fill=\"none\" points=\"22.5,-949.5 22.5,-985.5 140.5,-985.5 140.5,-949.5 22.5,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-963.8\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 120831999504&#45;&gt;120831999952 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>120831999504-&gt;120831999952</title>\n",
       "<path d=\"M81.5,-1022.4551C81.5,-1014.3828 81.5,-1004.6764 81.5,-995.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-995.5903 81.5,-985.5904 78.0001,-995.5904 85.0001,-995.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120832008768 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>120832008768</title>\n",
       "<polygon fill=\"none\" points=\"32,-876.5 32,-912.5 131,-912.5 131,-876.5 32,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-890.8\">conv1d_2: Conv1D</text>\n",
       "</g>\n",
       "<!-- 120831999952&#45;&gt;120832008768 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>120831999952-&gt;120832008768</title>\n",
       "<path d=\"M81.5,-949.4551C81.5,-941.3828 81.5,-931.6764 81.5,-922.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-922.5903 81.5,-912.5904 78.0001,-922.5904 85.0001,-922.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120832009888 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>120832009888</title>\n",
       "<polygon fill=\"none\" points=\"22.5,-803.5 22.5,-839.5 140.5,-839.5 140.5,-803.5 22.5,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-817.8\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 120832008768&#45;&gt;120832009888 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>120832008768-&gt;120832009888</title>\n",
       "<path d=\"M81.5,-876.4551C81.5,-868.3828 81.5,-858.6764 81.5,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-849.5903 81.5,-839.5904 78.0001,-849.5904 85.0001,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120832164136 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>120832164136</title>\n",
       "<polygon fill=\"none\" points=\"0,-730.5 0,-766.5 163,-766.5 163,-730.5 0,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-744.8\">max_pooling1d_1: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 120832009888&#45;&gt;120832164136 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>120832009888-&gt;120832164136</title>\n",
       "<path d=\"M81.5,-803.4551C81.5,-795.3828 81.5,-785.6764 81.5,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-776.5903 81.5,-766.5904 78.0001,-776.5904 85.0001,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120832164080 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>120832164080</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-657.5 31.5,-693.5 131.5,-693.5 131.5,-657.5 31.5,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-671.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 120832164136&#45;&gt;120832164080 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>120832164136-&gt;120832164080</title>\n",
       "<path d=\"M81.5,-730.4551C81.5,-722.3828 81.5,-712.6764 81.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-703.5903 81.5,-693.5904 78.0001,-703.5904 85.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120832164024 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>120832164024</title>\n",
       "<polygon fill=\"none\" points=\"32,-584.5 32,-620.5 131,-620.5 131,-584.5 32,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-598.8\">conv1d_3: Conv1D</text>\n",
       "</g>\n",
       "<!-- 120832164080&#45;&gt;120832164024 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>120832164080-&gt;120832164024</title>\n",
       "<path d=\"M81.5,-657.4551C81.5,-649.3828 81.5,-639.6764 81.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-630.5903 81.5,-620.5904 78.0001,-630.5904 85.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120832008936 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>120832008936</title>\n",
       "<polygon fill=\"none\" points=\"22.5,-511.5 22.5,-547.5 140.5,-547.5 140.5,-511.5 22.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-525.8\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 120832164024&#45;&gt;120832008936 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>120832164024-&gt;120832008936</title>\n",
       "<path d=\"M81.5,-584.4551C81.5,-576.3828 81.5,-566.6764 81.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-557.5903 81.5,-547.5904 78.0001,-557.5904 85.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4471134528 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>4471134528</title>\n",
       "<polygon fill=\"none\" points=\"32,-438.5 32,-474.5 131,-474.5 131,-438.5 32,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-452.8\">conv1d_4: Conv1D</text>\n",
       "</g>\n",
       "<!-- 120832008936&#45;&gt;4471134528 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>120832008936-&gt;4471134528</title>\n",
       "<path d=\"M81.5,-511.4551C81.5,-503.3828 81.5,-493.6764 81.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-484.5903 81.5,-474.5904 78.0001,-484.5904 85.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4471008952 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>4471008952</title>\n",
       "<polygon fill=\"none\" points=\"22.5,-365.5 22.5,-401.5 140.5,-401.5 140.5,-365.5 22.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-379.8\">activation_4: Activation</text>\n",
       "</g>\n",
       "<!-- 4471134528&#45;&gt;4471008952 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>4471134528-&gt;4471008952</title>\n",
       "<path d=\"M81.5,-438.4551C81.5,-430.3828 81.5,-420.6764 81.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-411.5903 81.5,-401.5904 78.0001,-411.5904 85.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120840089728 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>120840089728</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 163,-328.5 163,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-306.8\">max_pooling1d_2: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 4471008952&#45;&gt;120840089728 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>4471008952-&gt;120840089728</title>\n",
       "<path d=\"M81.5,-365.4551C81.5,-357.3828 81.5,-347.6764 81.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-338.5903 81.5,-328.5904 78.0001,-338.5904 85.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120840091240 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>120840091240</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-219.5 31.5,-255.5 131.5,-255.5 131.5,-219.5 31.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-233.8\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 120840089728&#45;&gt;120840091240 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>120840089728-&gt;120840091240</title>\n",
       "<path d=\"M81.5,-292.4551C81.5,-284.3828 81.5,-274.6764 81.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-265.5903 81.5,-255.5904 78.0001,-265.5904 85.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120840090232 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>120840090232</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-146.5 37.5,-182.5 125.5,-182.5 125.5,-146.5 37.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-160.8\">flatten_1: Flatten</text>\n",
       "</g>\n",
       "<!-- 120840091240&#45;&gt;120840090232 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>120840091240-&gt;120840090232</title>\n",
       "<path d=\"M81.5,-219.4551C81.5,-211.3828 81.5,-201.6764 81.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-192.5903 81.5,-182.5904 78.0001,-192.5904 85.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4471007832 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>4471007832</title>\n",
       "<polygon fill=\"none\" points=\"40,-73.5 40,-109.5 123,-109.5 123,-73.5 40,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-87.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 120840090232&#45;&gt;4471007832 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>120840090232-&gt;4471007832</title>\n",
       "<path d=\"M81.5,-146.4551C81.5,-138.3828 81.5,-128.6764 81.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-119.5903 81.5,-109.5904 78.0001,-119.5904 85.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120840249240 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>120840249240</title>\n",
       "<polygon fill=\"none\" points=\"22.5,-.5 22.5,-36.5 140.5,-36.5 140.5,-.5 22.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-14.8\">activation_5: Activation</text>\n",
       "</g>\n",
       "<!-- 4471007832&#45;&gt;120840249240 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>4471007832-&gt;120840249240</title>\n",
       "<path d=\"M81.5,-73.4551C81.5,-65.3828 81.5,-55.6764 81.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-46.5903 81.5,-36.5904 78.0001,-46.5904 85.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 120832008544 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>120832008544</title>\n",
       "<polygon fill=\"none\" points=\"41.5,-1095.5 41.5,-1131.5 121.5,-1131.5 121.5,-1095.5 41.5,-1095.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-1109.8\">120832008544</text>\n",
       "</g>\n",
       "<!-- 120832008544&#45;&gt;120831999504 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>120832008544-&gt;120831999504</title>\n",
       "<path d=\"M81.5,-1095.4551C81.5,-1087.3828 81.5,-1077.6764 81.5,-1068.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-1068.5903 81.5,-1058.5904 78.0001,-1068.5904 85.0001,-1068.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6392 samples, validate on 328 samples\n",
      "Epoch 1/100\n",
      "6392/6392 [==============================] - 3s 490us/step - loss: 9.7426 - acc: 0.2198 - val_loss: 4.2777 - val_acc: 0.2439\n",
      "Epoch 2/100\n",
      "6392/6392 [==============================] - 3s 448us/step - loss: 6.6238 - acc: 0.2171 - val_loss: 1.8140 - val_acc: 0.2439\n",
      "Epoch 3/100\n",
      "6392/6392 [==============================] - 3s 443us/step - loss: 2.8477 - acc: 0.2167 - val_loss: 1.7186 - val_acc: 0.2683\n",
      "Epoch 4/100\n",
      "6392/6392 [==============================] - 3s 437us/step - loss: 1.9085 - acc: 0.2369 - val_loss: 1.7396 - val_acc: 0.2744\n",
      "Epoch 5/100\n",
      "6392/6392 [==============================] - 3s 452us/step - loss: 1.7433 - acc: 0.2625 - val_loss: 1.7326 - val_acc: 0.3079\n",
      "Epoch 6/100\n",
      "6392/6392 [==============================] - 3s 447us/step - loss: 1.7092 - acc: 0.2674 - val_loss: 1.7156 - val_acc: 0.3140\n",
      "Epoch 7/100\n",
      "6392/6392 [==============================] - 3s 451us/step - loss: 1.6898 - acc: 0.2814 - val_loss: 1.7235 - val_acc: 0.3293\n",
      "Epoch 8/100\n",
      "6392/6392 [==============================] - 3s 442us/step - loss: 1.6794 - acc: 0.2879 - val_loss: 1.7132 - val_acc: 0.3110\n",
      "Epoch 9/100\n",
      "6392/6392 [==============================] - 3s 454us/step - loss: 1.6662 - acc: 0.3021 - val_loss: 1.6992 - val_acc: 0.3232\n",
      "Epoch 10/100\n",
      "6392/6392 [==============================] - 3s 460us/step - loss: 1.6576 - acc: 0.3054 - val_loss: 1.7133 - val_acc: 0.3171\n",
      "Epoch 11/100\n",
      "6392/6392 [==============================] - 3s 455us/step - loss: 1.6551 - acc: 0.3024 - val_loss: 1.6938 - val_acc: 0.3232\n",
      "Epoch 12/100\n",
      "6392/6392 [==============================] - 3s 459us/step - loss: 1.6479 - acc: 0.3085 - val_loss: 1.6945 - val_acc: 0.3384\n",
      "Epoch 13/100\n",
      "6392/6392 [==============================] - 3s 450us/step - loss: 1.6436 - acc: 0.3135 - val_loss: 1.7000 - val_acc: 0.3293\n",
      "Epoch 14/100\n",
      "6392/6392 [==============================] - 3s 458us/step - loss: 1.6430 - acc: 0.3135 - val_loss: 1.7027 - val_acc: 0.3171\n",
      "Epoch 15/100\n",
      "6392/6392 [==============================] - 3s 452us/step - loss: 1.6448 - acc: 0.3135 - val_loss: 1.6819 - val_acc: 0.3201\n",
      "Epoch 16/100\n",
      "6392/6392 [==============================] - 3s 446us/step - loss: 1.6421 - acc: 0.3218 - val_loss: 1.6988 - val_acc: 0.3110\n",
      "Epoch 17/100\n",
      "6392/6392 [==============================] - 3s 450us/step - loss: 1.6399 - acc: 0.3135 - val_loss: 1.7075 - val_acc: 0.3293\n",
      "Epoch 18/100\n",
      "6392/6392 [==============================] - 3s 436us/step - loss: 1.6302 - acc: 0.3293 - val_loss: 1.7031 - val_acc: 0.3140\n",
      "Epoch 19/100\n",
      "6392/6392 [==============================] - 3s 464us/step - loss: 1.6324 - acc: 0.3227 - val_loss: 1.7158 - val_acc: 0.3140\n",
      "Epoch 20/100\n",
      "6392/6392 [==============================] - 3s 459us/step - loss: 1.6299 - acc: 0.3204 - val_loss: 1.6822 - val_acc: 0.2957\n",
      "Epoch 21/100\n",
      "6392/6392 [==============================] - 3s 449us/step - loss: 1.6298 - acc: 0.3185 - val_loss: 1.7045 - val_acc: 0.3171\n",
      "Epoch 22/100\n",
      "6392/6392 [==============================] - 3s 450us/step - loss: 1.6253 - acc: 0.3271 - val_loss: 1.6697 - val_acc: 0.3110\n",
      "Epoch 23/100\n",
      "6392/6392 [==============================] - 3s 450us/step - loss: 1.6290 - acc: 0.3237 - val_loss: 1.6801 - val_acc: 0.3201\n",
      "Epoch 24/100\n",
      "6392/6392 [==============================] - 3s 440us/step - loss: 1.6237 - acc: 0.3293 - val_loss: 1.7040 - val_acc: 0.3110\n",
      "Epoch 25/100\n",
      "6392/6392 [==============================] - 3s 456us/step - loss: 1.6282 - acc: 0.3245 - val_loss: 1.6987 - val_acc: 0.3140\n",
      "Epoch 26/100\n",
      "6392/6392 [==============================] - 3s 458us/step - loss: 1.6258 - acc: 0.3243 - val_loss: 1.7167 - val_acc: 0.3079\n",
      "Epoch 27/100\n",
      "6392/6392 [==============================] - 3s 452us/step - loss: 1.6273 - acc: 0.3348 - val_loss: 1.7152 - val_acc: 0.3018\n",
      "Epoch 28/100\n",
      "6392/6392 [==============================] - 3s 454us/step - loss: 1.6239 - acc: 0.3296 - val_loss: 1.7198 - val_acc: 0.3110\n",
      "Epoch 29/100\n",
      "6392/6392 [==============================] - 3s 445us/step - loss: 1.6249 - acc: 0.3270 - val_loss: 1.7107 - val_acc: 0.3110\n",
      "Epoch 30/100\n",
      "6392/6392 [==============================] - 3s 456us/step - loss: 1.6187 - acc: 0.3293 - val_loss: 1.6955 - val_acc: 0.3354\n",
      "Epoch 31/100\n",
      "6392/6392 [==============================] - 3s 467us/step - loss: 1.6239 - acc: 0.3318 - val_loss: 1.7082 - val_acc: 0.2988\n",
      "Epoch 32/100\n",
      "6392/6392 [==============================] - 3s 468us/step - loss: 1.6203 - acc: 0.3315 - val_loss: 1.7128 - val_acc: 0.3293\n",
      "Epoch 33/100\n",
      "6392/6392 [==============================] - 3s 447us/step - loss: 1.6182 - acc: 0.3301 - val_loss: 1.7375 - val_acc: 0.2896\n",
      "Epoch 34/100\n",
      "6392/6392 [==============================] - 3s 459us/step - loss: 1.6165 - acc: 0.3315 - val_loss: 1.7231 - val_acc: 0.2988\n",
      "Epoch 35/100\n",
      "6392/6392 [==============================] - 3s 465us/step - loss: 1.6204 - acc: 0.3378 - val_loss: 1.7141 - val_acc: 0.3079\n",
      "Epoch 36/100\n",
      "6392/6392 [==============================] - 3s 449us/step - loss: 1.6189 - acc: 0.3367 - val_loss: 1.7037 - val_acc: 0.3110\n",
      "Epoch 37/100\n",
      "6392/6392 [==============================] - 3s 444us/step - loss: 1.6190 - acc: 0.3357 - val_loss: 1.7137 - val_acc: 0.3018\n",
      "Epoch 38/100\n",
      "6392/6392 [==============================] - 3s 455us/step - loss: 1.6184 - acc: 0.3292 - val_loss: 1.7432 - val_acc: 0.2896\n",
      "Epoch 39/100\n",
      "6392/6392 [==============================] - 3s 480us/step - loss: 1.6110 - acc: 0.3343 - val_loss: 1.7258 - val_acc: 0.2927\n",
      "Epoch 40/100\n",
      "6392/6392 [==============================] - 3s 485us/step - loss: 1.6120 - acc: 0.3375 - val_loss: 1.7452 - val_acc: 0.3018\n",
      "Epoch 41/100\n",
      "6392/6392 [==============================] - 3s 489us/step - loss: 1.6154 - acc: 0.3373 - val_loss: 1.7080 - val_acc: 0.3079\n",
      "Epoch 42/100\n",
      "6392/6392 [==============================] - 3s 488us/step - loss: 1.6117 - acc: 0.3334 - val_loss: 1.7406 - val_acc: 0.3018\n",
      "Epoch 43/100\n",
      "6392/6392 [==============================] - 3s 494us/step - loss: 1.6201 - acc: 0.3334 - val_loss: 1.7357 - val_acc: 0.3049\n",
      "Epoch 44/100\n",
      "6392/6392 [==============================] - 3s 488us/step - loss: 1.6141 - acc: 0.3364 - val_loss: 1.7391 - val_acc: 0.3079\n",
      "Epoch 45/100\n",
      "6392/6392 [==============================] - 3s 494us/step - loss: 1.6138 - acc: 0.3348 - val_loss: 1.7478 - val_acc: 0.2744\n",
      "Epoch 46/100\n",
      "6392/6392 [==============================] - 3s 486us/step - loss: 1.6147 - acc: 0.3328 - val_loss: 1.7583 - val_acc: 0.2805\n",
      "Epoch 47/100\n",
      "6392/6392 [==============================] - 3s 482us/step - loss: 1.6148 - acc: 0.3348 - val_loss: 1.7474 - val_acc: 0.3140\n",
      "Epoch 48/100\n",
      "6392/6392 [==============================] - 3s 480us/step - loss: 1.6142 - acc: 0.3353 - val_loss: 1.7577 - val_acc: 0.2957\n",
      "Epoch 49/100\n",
      "6392/6392 [==============================] - 3s 487us/step - loss: 1.6175 - acc: 0.3323 - val_loss: 1.7512 - val_acc: 0.2957\n",
      "Epoch 50/100\n",
      "6392/6392 [==============================] - 3s 479us/step - loss: 1.6157 - acc: 0.3370 - val_loss: 1.7432 - val_acc: 0.3018\n",
      "Epoch 51/100\n",
      "6392/6392 [==============================] - 3s 492us/step - loss: 1.6126 - acc: 0.3451 - val_loss: 1.7586 - val_acc: 0.2927\n",
      "Epoch 52/100\n",
      "6392/6392 [==============================] - 3s 491us/step - loss: 1.6128 - acc: 0.3314 - val_loss: 1.7417 - val_acc: 0.2866\n",
      "Epoch 53/100\n",
      "6392/6392 [==============================] - 3s 498us/step - loss: 1.6117 - acc: 0.3396 - val_loss: 1.7483 - val_acc: 0.3110\n",
      "Epoch 54/100\n",
      "6392/6392 [==============================] - 3s 484us/step - loss: 1.6111 - acc: 0.3328 - val_loss: 1.7439 - val_acc: 0.2835\n",
      "Epoch 55/100\n",
      "6392/6392 [==============================] - 3s 477us/step - loss: 1.6118 - acc: 0.3364 - val_loss: 1.7515 - val_acc: 0.2835\n",
      "Epoch 56/100\n",
      "6392/6392 [==============================] - 3s 486us/step - loss: 1.6141 - acc: 0.3406 - val_loss: 1.7490 - val_acc: 0.3171\n",
      "Epoch 57/100\n",
      "6392/6392 [==============================] - 3s 487us/step - loss: 1.6109 - acc: 0.3470 - val_loss: 1.7383 - val_acc: 0.2988\n",
      "Epoch 58/100\n",
      "6392/6392 [==============================] - 3s 487us/step - loss: 1.6096 - acc: 0.3357 - val_loss: 1.7436 - val_acc: 0.2835\n",
      "Epoch 59/100\n",
      "6392/6392 [==============================] - 3s 487us/step - loss: 1.6159 - acc: 0.3389 - val_loss: 1.7527 - val_acc: 0.2927\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6392/6392 [==============================] - 3s 455us/step - loss: 1.6119 - acc: 0.3404 - val_loss: 1.7539 - val_acc: 0.2957\n",
      "Epoch 61/100\n",
      "6392/6392 [==============================] - 3s 434us/step - loss: 1.6144 - acc: 0.3335 - val_loss: 1.7716 - val_acc: 0.2835\n",
      "Epoch 62/100\n",
      "6392/6392 [==============================] - 3s 442us/step - loss: 1.6119 - acc: 0.3373 - val_loss: 1.7570 - val_acc: 0.3018\n",
      "Epoch 63/100\n",
      "6392/6392 [==============================] - 3s 456us/step - loss: 1.6146 - acc: 0.3395 - val_loss: 1.7626 - val_acc: 0.2744\n",
      "Epoch 64/100\n",
      "6392/6392 [==============================] - 3s 444us/step - loss: 1.6077 - acc: 0.3437 - val_loss: 1.7495 - val_acc: 0.2957\n",
      "Epoch 65/100\n",
      "6392/6392 [==============================] - 3s 447us/step - loss: 1.6107 - acc: 0.3406 - val_loss: 1.7808 - val_acc: 0.2774\n",
      "Epoch 66/100\n",
      "6392/6392 [==============================] - 3s 477us/step - loss: 1.6084 - acc: 0.3439 - val_loss: 1.7790 - val_acc: 0.2774\n",
      "Epoch 67/100\n",
      "6392/6392 [==============================] - 3s 473us/step - loss: 1.6101 - acc: 0.3414 - val_loss: 1.7440 - val_acc: 0.2866\n",
      "Epoch 68/100\n",
      "6392/6392 [==============================] - 3s 478us/step - loss: 1.6131 - acc: 0.3417 - val_loss: 1.7647 - val_acc: 0.2957\n",
      "Epoch 69/100\n",
      "6392/6392 [==============================] - 3s 471us/step - loss: 1.6107 - acc: 0.3348 - val_loss: 1.7788 - val_acc: 0.2866\n",
      "Epoch 70/100\n",
      "6392/6392 [==============================] - 3s 465us/step - loss: 1.6092 - acc: 0.3379 - val_loss: 1.7719 - val_acc: 0.2774\n",
      "Epoch 71/100\n",
      "6392/6392 [==============================] - 3s 476us/step - loss: 1.6087 - acc: 0.3387 - val_loss: 1.7680 - val_acc: 0.2805\n",
      "Epoch 72/100\n",
      "6392/6392 [==============================] - 3s 472us/step - loss: 1.6116 - acc: 0.3398 - val_loss: 1.7937 - val_acc: 0.2774\n",
      "Epoch 73/100\n",
      "6392/6392 [==============================] - 3s 443us/step - loss: 1.6165 - acc: 0.3365 - val_loss: 1.7777 - val_acc: 0.2988\n",
      "Epoch 74/100\n",
      "6392/6392 [==============================] - 3s 458us/step - loss: 1.6116 - acc: 0.3364 - val_loss: 1.7759 - val_acc: 0.2988\n",
      "Epoch 75/100\n",
      "6392/6392 [==============================] - 3s 445us/step - loss: 1.6128 - acc: 0.3401 - val_loss: 1.7953 - val_acc: 0.3079\n",
      "Epoch 76/100\n",
      "6392/6392 [==============================] - 3s 453us/step - loss: 1.6098 - acc: 0.3385 - val_loss: 1.7902 - val_acc: 0.3018\n",
      "Epoch 77/100\n",
      "6392/6392 [==============================] - 3s 454us/step - loss: 1.6085 - acc: 0.3385 - val_loss: 1.7756 - val_acc: 0.2896\n",
      "Epoch 78/100\n",
      "6392/6392 [==============================] - 3s 459us/step - loss: 1.6106 - acc: 0.3401 - val_loss: 1.7444 - val_acc: 0.2896\n",
      "Epoch 79/100\n",
      "6392/6392 [==============================] - 3s 479us/step - loss: 1.6109 - acc: 0.3445 - val_loss: 1.7825 - val_acc: 0.2652\n",
      "Epoch 80/100\n",
      "6392/6392 [==============================] - 3s 482us/step - loss: 1.6089 - acc: 0.3415 - val_loss: 1.7615 - val_acc: 0.3079\n",
      "Epoch 81/100\n",
      "6392/6392 [==============================] - 3s 480us/step - loss: 1.6120 - acc: 0.3342 - val_loss: 1.7646 - val_acc: 0.2957\n",
      "Epoch 82/100\n",
      "6392/6392 [==============================] - 3s 487us/step - loss: 1.6079 - acc: 0.3456 - val_loss: 1.7566 - val_acc: 0.2957\n",
      "Epoch 83/100\n",
      "6392/6392 [==============================] - 3s 488us/step - loss: 1.6080 - acc: 0.3429 - val_loss: 1.7791 - val_acc: 0.2957\n",
      "Epoch 84/100\n",
      "6392/6392 [==============================] - 3s 473us/step - loss: 1.6117 - acc: 0.3362 - val_loss: 1.7719 - val_acc: 0.2927\n",
      "Epoch 85/100\n",
      "6392/6392 [==============================] - 3s 452us/step - loss: 1.6053 - acc: 0.3425 - val_loss: 1.7813 - val_acc: 0.2896\n",
      "Epoch 86/100\n",
      "6392/6392 [==============================] - 3s 472us/step - loss: 1.6084 - acc: 0.3395 - val_loss: 1.7833 - val_acc: 0.2835\n",
      "Epoch 87/100\n",
      "6392/6392 [==============================] - 3s 478us/step - loss: 1.6075 - acc: 0.3345 - val_loss: 1.7573 - val_acc: 0.2988\n",
      "Epoch 88/100\n",
      "6392/6392 [==============================] - 3s 485us/step - loss: 1.6136 - acc: 0.3310 - val_loss: 1.7975 - val_acc: 0.2896\n",
      "Epoch 89/100\n",
      "6392/6392 [==============================] - 3s 486us/step - loss: 1.6068 - acc: 0.3371 - val_loss: 1.7832 - val_acc: 0.2835\n",
      "Epoch 90/100\n",
      "6392/6392 [==============================] - 3s 490us/step - loss: 1.6096 - acc: 0.3382 - val_loss: 1.7958 - val_acc: 0.2988\n",
      "Epoch 91/100\n",
      "6392/6392 [==============================] - 3s 494us/step - loss: 1.6062 - acc: 0.3439 - val_loss: 1.7738 - val_acc: 0.3018\n",
      "Epoch 92/100\n",
      "6392/6392 [==============================] - 3s 482us/step - loss: 1.6120 - acc: 0.3432 - val_loss: 1.7721 - val_acc: 0.2835\n",
      "Epoch 93/100\n",
      "6392/6392 [==============================] - 3s 487us/step - loss: 1.6099 - acc: 0.3387 - val_loss: 1.7831 - val_acc: 0.2866\n",
      "Epoch 94/100\n",
      "6392/6392 [==============================] - 3s 483us/step - loss: 1.6076 - acc: 0.3418 - val_loss: 1.7744 - val_acc: 0.2957\n",
      "Epoch 95/100\n",
      "6392/6392 [==============================] - 3s 492us/step - loss: 1.6099 - acc: 0.3476 - val_loss: 1.7942 - val_acc: 0.3171\n",
      "Epoch 96/100\n",
      "6392/6392 [==============================] - 3s 481us/step - loss: 1.6175 - acc: 0.3393 - val_loss: 1.7712 - val_acc: 0.2866\n",
      "Epoch 97/100\n",
      "6392/6392 [==============================] - 3s 489us/step - loss: 1.6076 - acc: 0.3406 - val_loss: 1.7867 - val_acc: 0.3110\n",
      "Epoch 98/100\n",
      "6392/6392 [==============================] - 3s 497us/step - loss: 1.6108 - acc: 0.3420 - val_loss: 1.7927 - val_acc: 0.2744\n",
      "Epoch 99/100\n",
      "6392/6392 [==============================] - 3s 490us/step - loss: 1.6144 - acc: 0.3446 - val_loss: 1.7652 - val_acc: 0.3201\n",
      "Epoch 100/100\n",
      "6392/6392 [==============================] - 3s 486us/step - loss: 1.6051 - acc: 0.3439 - val_loss: 1.7982 - val_acc: 0.3079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1ec9cb38>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing extraneous layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 5, padding='same',\n",
    "                 input_shape=(272,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(32, 5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(64, 5, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
